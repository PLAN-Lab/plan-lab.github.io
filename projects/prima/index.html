<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation.">
  <meta name="keywords" content="PRIMA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-X6K9XMSJM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-X6K9XMSJM2');
  </script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/prima_ballerina.svg" alt="PRIMA logo"
                width="80px"><span class="model-name">PRIMA</span>: Multi-Image Vision-Language Models for Reasoning
              Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <div>
                <span class="author-block">
                  <a href="https://mwahed.com/">Muntasir Wahed<sup class="illini-orange">1*</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/kanguyen-vn/">Kiet A. Nguyen<sup
                      class="illini-orange">1*</sup></a>,
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/juvekaradheesh/">Adheesh Juvekar<sup
                        class="illini-orange">1</sup></a>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/xinzhuo-li">Xinzhuo Li<sup class="illini-orange">1</sup></a>,
                  </span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/xnz">Xiaona Zhou<sup class="illini-orange">1</sup></a>,
                  </span>
              </div>
              <div>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/shahvedant">Vedant Shah<sup class="illini-orange">1</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://tianjiao-yu.github.io/">Tianjiao Yu<sup class="illini-orange">1</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://pinguar.org/">Pinar Yanardag<sup class="vt-maroon">2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://isminoula.github.io/">Ismini Lourentzou<sup class="illini-orange">1</sup></a>
                </span>
              </div>
            </div>

            <div class="is-size-5 publication-authors">
              <!--<span class="author-block">PLAN Lab</span>,-->
              <span class="author-block illini-orange"><sup>1</sup>University of Illinois Urbana-Champaign</span>,
              <span class="author-block vt-maroon"><sup>2</sup>Virginia Tech</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="illini-blue"><span class="illini-orange">*</span> Equal Contribution</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.15209" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.15209" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data (coming soon)</span>
                  </a>
                </span>
                <!-- Demo Link. -->
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-vr-cardboard"></i>
                    </span>
                    <span>Demo (coming soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="content has-text-justified">
          <span style="font-weight: bold;">TL;DR:</span> We introduce
          the task of <span style="font-style: italic;">multi-image pixel-grounded reasoning segmentation</span>,
          with the release of the <span class="model-name">M<sup>4</sup>Seg</span> benchmark consisting of ‚àº744K
          question-answer pairs
          that require fine-grained visual understanding across multiple images,
          and the <span class="model-name">PRIMA</span> LVLM model that combines
          pixel-level grounding with multi-image reasoning capabilities.
        </div>
        <img src="./static/images/prima_teaser.png" class="interpolation-image"
          alt="Interpolate start reference image." />
        <div class="content has-text-justified">
          We introduce the new task of 
          <span style="font-style: italic; font-weight: bold;">multi-image pixel-grounded reasoning segmentation</span>. 
          To support this task, we curate <span class="model-name">M<sup>4</sup>Seg</span>, 
          a benchmark providing question-answer (QA) pairs alongside image sets with 
          pixel-level annotations. Additionally, we propose 
          <span class="model-name">PRIMA</span>, a model designed to efficiently identify 
          and compare objects' contextual relationships across scenes. 
          We focus on four key categories essential for multi-image understanding: functional, spatial, numerical, and open-ended reasoning.
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">üìù Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite significant advancements in Large Vision-Language Models (LVLMs)' capabilities, 
              existing pixel-grounding models operate in single-image settings, 
              limiting their ability to perform detailed, fine-grained comparisons across 
              multiple images. Conversely, current multi-image understanding models lack 
              pixel-level grounding. Our work addresses this gap by introducing the task of 
              <span style="font-style: italic; font-weight: bold;">multi-image pixel-grounded reasoning
                segmentation</span> alongside <span class="model-name">PRIMA</span>, 
                an LVLM that integrates pixel-level grounding with robust multi-image reasoning 
                to produce contextually rich, pixel-grounded explanations. 
                Central to <span class="model-name">PRIMA</span> is <span class="model-name">SQuARE</span>, 
                a vision module that injects cross-image relational context into compact query-based 
                visual tokens before fusing them with the language backbone. 
                To support training and evaluation, we curate <span class="model-name">M<sup>4</sup>Seg</span>, a new multi-image reasoning 
                segmentation benchmark consisting of ~744K question-answer pairs that require 
                fine-grained visual understanding across multiple images. 
                <span class="model-name">PRIMA</span> outperforms state-of-the-art baselines with 7.83% and 11.25% improvements 
                in Recall and S-IoU, respectively. Ablation studies further demonstrate the 
                effectiveness of the proposed SQuARE module in capturing cross-image relationships.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section" style="background-color:#dadada81">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">üí° Contributions</h2>
          <div class="content has-text-justified">
            <ul class="fa-ul">
              <li><span class="fa-li"><i class="fa fa-star" style="color:#FFD700"></i></span><b>Novel Task</b>. We
                propose the novel task of
                <span style="font-style: italic; font-weight: bold;">multi-image pixel-grounded reasoning
                  segmentation</span>, which requires fine-grained comparison and contextual 
                  understanding across multiple images at pixel level and natural language reasoning.
              </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-star" style="color:#FFD700"></i></span><b>New Benchmark</b>. We
                curate
                <span class="model-name">M<sup>4</sup>Seg</span>, a new challenging benchmark with ~744K multi-image QA 
                pairs, annotated with multiple object and part segmentation masks to train and evaluate 
                multi-image pixel-grounding models.
              </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-star" style="color:#FFD700"></i></span><b>New Multi-Image
                  Pixel-Grounded LVLM</b>. We propose
                <span class="model-name">PRIMA</span>,
                an LVLM designed to perform instruction-guided cross-image alignment of relevant visual features via a 
                novel <span class="model-name">SQuARE</span> module, enabling reasoning with contextually grounded segmentation masks across multiple 
                images. Experiments demonstrate <span class="model-name">PRIMA</span>'s impressive performance 
                compared to strong baselines across segmentation (<span style="color: #6B8E23; font-weight: bold;">+‚Üë</span>8.11% mIoU 
                and <span style="color: #6B8E23; font-weight: bold;">+‚Üë</span>7.83% Recall) and text-based metrics 
                (<span style="color: #6B8E23; font-weight: bold;">+‚Üë</span>6.45% Semantic Similarity and 
                <span style="color: #6B8E23; font-weight: bold;">+‚Üë</span>11.25% S-IoU). 
              </li>
              </ol>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name"><img src="static/images/prima_ballerina.svg" alt="PRIMA logo"
                width="40px"> PRIMA</span> Architecture</h2>
          <div class="content has-text-justified">
            <img src="./static/images/prima_arch.png" class="interpolation-image" alt="PRIMA Model Architecture." />
            <p>
              Overview of the proposed <span class="model-name">PRIMA</span> architecture. Leveraging a LoRA-finetuned language model, a novel SQUARE vision encoder, and a SAM-based decoder, PRIMA dynamically generates segmentation masks corresponding to objects referenced in natural language queries, supporting pixel-grounded reasoning in complex multi-image tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name"><img src="static/images/prima_ballerina.svg" alt="PRIMA logo"
                width="40px"> SQuARE</span> module</h2>
          <div class="content has-text-justified">
            <div style="text-align: center;">
              <img src="./static/images/prima_arch_square.png" class="interpolation-image" alt="SQuARE Module." width="50%" />
            </div>
            <p>
              Our proposed <span class="model-name">SQuARE</span> module. Learnable relational queries attend over the concatenated multi-image features to form a shared relational representation. This representation is injected into the query pathway for global feature extraction, producing enriched visual representations that capture cross-image interactions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">üìä Quantitative Results</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; padding: 0 0 20px 0;">
              <img src="./static/images/results.png" class="interpolation-image" alt="PRIMA results." width="85%" />
              <p class="has-text-justified">
                Experimental Results on <span class="model-name">M<sup>4</sup>Seg</span>. 
                <span class="model-name">PRIMA</span> significantly outperforms both general-purpose and pixel-grounding LVLM 
                baselines. The general-purpose LVLMs struggle with this task, a limitation we attribute 
                to their lack of task-specific training and cascading errors from using SAM on the text-based 
                grounding information. While Gemini-2.5 Pro is the top performer in this category, its 
                performance remains limited with 30.21% mIoU and 68.51% SS. The reasoning segmentation 
                baselines perform better, as they leverage pixel grounding capabilities and are finetuned on 
                <span class="model-name">M<sup>4</sup>Seg</span>. GLaMM, for instance, outperforms the general-purpose LVLMs with 38.12% 
                mIoU and 74.05% SS. Compared to all baselines, <span class="model-name">PRIMA</span> sets a new benchmark, surpassing 
                the next best baseline by 8.11% and 12.33% in terms of mIoU and I-SS, respectively. 
                <span class="model-name">PRIMA</span> also achieves a gain of up to 6.45% and 11.25% on the text metrics SS and SIoU.
              </p>

            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">üîé Qualitative Examples</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; padding: 0 0 20px 0;">
              <img src="./static/images/qualitative.png" class="interpolation-image"
                alt="Qualitative Examples." width="85%" />
            </div>
          </div>
          <div class="content has-text-justified">
            Qualitative Results. <span class="model-name">PRIMA</span> exhibits strong qualitative performance 
            in both segmentation and reasoning. Each example shows the posted question, the textual 
            responses from each model, and the corresponding segmentation masks alongside the Ground 
            Truth reference. For clarity, we use consistent colors between the segmentation masks 
            and the highlighted text spans referring to each object or part. The results indicate that 
            <span class="model-name">PRIMA</span> produces high-quality textual responses together with crisp, well-localized 
            segmentation masks, whereas GLaMM often yields noisy masks, attends to incorrect objects 
            (e.g., treating the cabinet as a surface for placing small items), and hallucinates 
            categories (e.g., misidentifying the horse as a truck or a dog). We also include a 
            failure case in which <span class="model-name">PRIMA</span> predicts the correct number of objects but fails to 
            precisely distinguish individual instances, instead producing overlapping masks.
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">üîé PRIMA in the Wild</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; padding: 0 0 20px 0;">
              <img src="./static/images/prima_in_the_wild.png" class="interpolation-image"
                alt="PRIMA in the wild results." width="85%" />
            </div>
          </div>
          <div class="content has-text-justified">
            <span class="model-name">PRIMA</span>'s performance on unseen images found on the web. 
            Notably, <span class="model-name">PRIMA</span> generates superior segmentation masks compared to baselines, 
            as exemplified by the fine-grained details of the hammers in Example 3 and the dining chairs 
            in Example 4. Moreover, the baselines are more prone to hallucinations, 
            e.g., "pizza is shown with multiple slices" in the first example (GLaMM), "watch" 
            in the second example (both GLaMM and LISA). Beyond visual recognition, these examples 
            underscore <span class="model-name">PRIMA</span>'s capability to retain compositional reasoning capabilities when 
            transferring to diverse and noisy real-world images.
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title" style="justify-content: left;display:flex;">BibTeX</h2>
      <pre><code>@article{wahed2024prima,
  title={PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation},
  author={Wahed, Muntasir and Nguyen, Kiet A and Juvekar, Adheesh Sunil and Li, Xinzhuo and Zhou, Xiaona and Shah, Vedant and Yu, Tianjiao and Yanardag, Pinar and Lourentzou, Ismini},
  journal={arXiv preprint arXiv:2412.15209},
  year={2024}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2412.15209">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/PLAN-Lab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This site is built upon the work of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              made available under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>. We gratefully acknowledge <a
                href="https://arxiv.org/pdf/2304.08485">LLaVA</a>,
              <a href="https://arxiv.org/pdf/2311.03356">GLaMM</a>,
              <a href="https://arxiv.org/pdf/2304.07193">DINOv2</a>, <a
                href="https://arxiv.org/pdf/2301.12597">Q-Former</a>, and <a
                href="https://arxiv.org/pdf/2308.04152">Cheetah</a> for open-sourcing their models
              and code.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>