<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs">
  <meta name="keywords" content="spatial reasoning, vision language models, preference optimization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .pdf-container {
      width: 100%;
      max-width: 800px;
      margin: 0 auto;
      border: none;
      border-radius: 8px;
      overflow: hidden;
    }
    .pdf-embed {
      width: 100%;
      height: 400px;
      border: none;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title" style="display: flex; align-items: center; justify-content: center; gap: 15px;"><img src="static/images/logo.png" alt="SpatialReasoner Logo" width="100px"><span>Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</span></h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yifanshens/" class="external-link"><span class="illini-blue">Yifan Shen</span></a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/yuanzhe-liu-upenn-ds/" class="external-link"><span class="illini-blue">Yuanzhe Liu</span></a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jingyuan-zhu-6a0ab3322/" class="external-link"><span class="illini-blue">Jingyuan Zhu</span></a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.irohxucao.com/" class="external-link"><span class="illini-blue">Xu Cao</span></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://zhangbaijin.github.io/" class="external-link"><span class="illini-blue">Xiaofeng Zhang</span></a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Mr-xiu" class="external-link"><span class="illini-blue">Yixiao He</span></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/wenming-ye-0170b11/" class="external-link"><span class="illini-blue">Wenming Ye</span></a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://rehg.org/" class="external-link"><span class="illini-blue">James Matthew Rehg</span></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://isminoula.github.io/" class="external-link"><span class="illini-blue">Ismini Lourentzou</span></a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors" style="margin-top: 10px;">
              <div>
                <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign</span>,
                <span class="author-block"><sup>2</sup>University of Pennsylvania</span>
              </div>
              <div>
                <span class="author-block"><sup>3</sup>Shanghai Jiao Tong University</span>,
                <span class="author-block"><sup>4</sup>Google</span>
              </div>
            </div>
            
            <div class="has-text-centered" style="margin: 5px 0;">
              <img src="./static/images/u_logo.png" alt="Affiliation Logos" style="height: 55px;">
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.21656" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.21656" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="content has-text-justified">
          <span style="font-weight: bold;">TL;DR:</span> We propose a novel fine-grained preference optimization approach that significantly improves spatial reasoning capabilities in Vision-Language Models (VLMs). Our method leverages carefully designed preference data and training strategies to enhance spatial understanding without compromising general visual capabilities.
        </div>
        <div class="pdf-container">
          <img src="./static/images/teaser.png" alt="Workflow Overview" style="width: 120%; border: none;">
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce <span style="color:rgb(26,78,138); font-weight:bold;">Spatial</span><span style="color:rgb(86,187,204); font-weight:bold;">Reasoner-R1</span>, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SpatialRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.           </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#dadada81">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">✅ Contributions</h2>
          <div class="content has-text-justified">
            <ul class="fa-ul">
              <li><span class="fa-li"><i class="fa fa-check"></i></span><b>SpatialReasoner-R1</b>. We introduce SpatialReasoner-R1, a VLM designed for fine-grained LongCoT spatial reasoning that effectively generates interpretable, step-by-step explanations directly from 2D images. SpatialReasoner-R1 establishes a new SoTA in spatial understanding tasks, while maintaining robust performance on general vision-language benchmarks.</li>
              <br>
              <li><span class="fa-li"><i class="fa fa-check"></i></span><b>Fine-grained Direct Preference Optimization</b>. To enhance training stability and precision, we propose a new fine-grained Direct Preference Optimization fDPO method that employs segment-specific learning updates tailored explicitly for descriptive grounding and logical reasoning.</li>
              <br>
              <li><span class="fa-li"><i class="fa fa-check"></i></span><b>Multi-Model Monte Carlo Tree Search</b>. To address the scarcity of high-quality spatial reasoning data, we introduce a data generation pipeline that combines Multi-Model Monte Carlo Tree Search  (M3CTS) with fine-grained spatial rewards, enabling the creation of diverse, logically consistent LongCoT trajectories for fine-grained preference training.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Method Overview</h2>
          <div class="content has-text-justified">
            <div class="pdf-container">
              <img src="./static/images/workflow_c.png" alt="Fine-Grained Preference Optimization" style="width: 120%; border: none;">
            </div>
            <div class="horizontal-container">
              <p>Method Overview including <span style="color:rgb(26,78,138); font-weight:bold;">Spatial</span><span style="color:rgb(86,187,204); font-weight:bold;">Reasoner-R1</span> model architecture and training pipeline. Training pipeline consisting of three stages: (1) generating reasoning paths using M3CTS; (2) constructing fine-grained preference pairs via reward-based selection; (3) training with fine-grained DPO (fDPO) to optimize descriptive and logical reasoning separately.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Fine-Grained Spatial Rewards</h2>
          <div class="content has-text-justified">
            <div class="pdf-container">
              <img src="./static/images/fdpo_c.png" alt="Fine-Grained Preference Optimization" style="width: 120%; border: none;">
            </div>
            <div class="horizontal-container">
                             <p>Fine-Grained Spatial Rewards. Candidate reasoning paths are decomposed into three aspects, <em>descriptive</em>, <em>spatial</em>, and <em>reasoning</em>, scored separately; the higher value in each row is marked by <span style="color: green;">✔</span> and the lower by <span style="color: red;">✖</span>.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Spatial Reasoning Evaluation</h2>
          <div class="content has-text-justified">
            <p>We conduct comprehensive evaluation on spatial reasoning tasks to demonstrate the effectiveness of our approach.</p>
            <div class="pdf-container" style="width: 65%; margin: 0 auto;">
              <img src="./static/images/result.png" alt="Comparison Results" style="width: 100%; border: none;">
            </div>
            <p>Spatial reasoning success rates (↑) on SpatialRGPT-Bench. "/" indicates that the model refuses to provide a response for that metric. <span style="color:rgb(26,78,138); font-weight:bold;">Spatial</span><span style="color:rgb(86,187,204); font-weight:bold;">Reasoner-R1</span> 8B, trained with fDPO, establishes a new SoTA in spatial reasoning.</p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Examples and Results</h2>
          <p class="has-text-justified" style="padding-bottom:30px;">
            <span style="color:rgb(26,78,138); font-weight:bold;">Spatial</span><span style="color:rgb(86,187,204); font-weight:bold;">Reasoner-R1</span> demonstrates improved spatial reasoning across various scenarios and object types:
          </p>
          
          <div style="text-align: center; padding: 0 0 30px 0;">
            <div class="pdf-container" style="width: 80%; margin: 0 auto;">
              <img src="./static/images/spatial_comparison_c.png" alt="Spatial Comparison" style="width: 100%; border: none;">
            </div>
            <p class="has-text-justified" style="margin-top: 20px;">
              <!-- Comparison of spatial reasoning performance across different models and approaches. -->
            </p>
          </div>

          <div style="text-align: center; padding: 0 0 20px 0;">
            <div class="pdf-container" style="width: 80%; margin: 0 auto;">
              <img src="./static/images/additionexample1.png" alt="Additional Examples" style="width: 100%; border: none;">
            </div>
            <p class="has-text-justified" style="margin-top: 20px;">
              <!-- Additional examples showing the improved spatial reasoning capabilities of our fine-grained preference optimized models. -->
            </p>
          </div>

          <div style="text-align: center; padding: 0 0 20px 0;">
            <div class="pdf-container" style="width: 80%; margin: 0 auto;">
              <img src="./static/images/additional_example2.png" alt="Additional Examples 2" style="width: 100%; border: none;">
            </div>
            <p class="has-text-justified" style="margin-top: 20px;">
              <!-- More examples demonstrating enhanced spatial reasoning capabilities through fine-grained preference optimization. -->
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{shen2025finegrainedpreferenceoptimizationimproves,
      title={Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs}, 
      author={Yifan Shen and Yuanzhe Liu and Jingyuan Zhu and Xu Cao and Xiaofeng Zhang and Yixiao He and Wenming Ye and James Matthew Rehg and Ismini Lourentzou},
      year={2025},
      eprint={2506.21656},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.21656}, 
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="#">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/PLAN-Lab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This site is built upon the work of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              made available under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer> 
</body>

</html>
