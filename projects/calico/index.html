<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models">
  <meta name="keywords" content="part segmentation, part co-segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>[CVPR'25] CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/calico.svg" alt="CALICO cat"
                width="80px"><span class="model-name">Calico</span>: Part-Focused Semantic Co-Segmentation with Large
              Vision-Language Models</h1>
            <h1 class="title is-3 publication-venue illini-orange">ðŸš€Accepted to CVPR 2025ðŸš€</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kanguyen-vn/" class="illini-blue">Kiet A. Nguyen</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/juvekaradheesh/" class="illini-blue">Adheesh Juvekar</a>,</span>
              <span class="author-block">
                <a href="https://tianjiao-yu.github.io/" class="illini-blue">Tianjiao Yu</a>,
              </span>
              <span class="author-block">
                <a href="https://mwahed.com/" class="illini-blue">Muntasir Wahed</a>,
              </span>
              <span class="author-block">
                <a href="https://isminoula.github.io/" class="illini-blue">Ismini Lourentzou</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">PLAN Lab</span>,
              <span class="author-block illini-orange">University of Illinois Urbana-Champaign</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.19331" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.19331" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data (coming soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-vr-cardboard"></i>
                    </span>
                    <span>Demo (coming soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="content has-text-justified">
          <span style="font-weight: bold;">TL;DR:</span> We present <span class="model-name">Calico</span>, the first
          LVLM
          designed for <span style="font-style: italic;">part-focused semantic co-segmentation</span>, a new task that
          identifies and
          segments common and unique object parts across multiple images. Trained on <span
            class="model-name">MixedParts</span>, our new dataset with ~2.4M samples across ~44K images,
          <span class="model-name">Calico</span> achieves strong
          performance in this domain with just 0.3% of its architecture finetuned.
        </div>
        <img src="./static/images/calico_teaser.svg" class="interpolation-image"
          alt="Interpolate start reference image." />
        <div class="content has-text-justified">
          Our proposed <span style="font-style: italic;">part-focused semantic
            co-segmentation</span> task, where the goal is to identify, segment, and label
          common objects, as well as common and unique object parts across multiple images.
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through
              visual instruction tuning.
              While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle
              with segmentation-grounded reasoning across images, especially at finer granularities such as object
              parts. In this paper, we introduce the new task of
              <span style="font-style: italic; font-weight: bold;">part-focused semantic co-segmentation</span>, which
              involves identifying and segmenting common objects, as well as common and unique object parts across
              images.
            </p>

            <p>To address this task, we present <span class="model-name">Calico</span>, the first LVLM
              designed for multi-image part-level reasoning segmentation. <span class="model-name">Calico</span>
              features two proposed components, a novel
              Correspondence Extraction
              Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that
              embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient
              manner. To support training and evaluation, we curate
              <span class="model-name">MixedParts</span>, a large-scale multi-image segmentation dataset containing
              ~2.4M samples across
              ~44K images spanning diverse object and part categories. Experimental results demonstrate that <span
                class="model-name">Calico</span>, with just 0.3% of its parameters finetuned, achieves strong
              performance on this challenging task.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#dadada81">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">âœ… Contributions</h2>
          <div class="content has-text-justified">
            <ul class="fa-ul">
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>Novel Task</b>. We introduce
                the novel task of <span style="font-style: italic;">part-focused
                  semantic co-segmentation</span>, which aims to co-segment and label common and unique parts between
                objects across images for granular object comparison. To the best of our
                knowledge, this is the first work to formalize this multi-image object/part co-segmentation task. </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>New Multi-Image
                  Pixel-Grounded LVLM</b>. We propose <span class="model-name">Calico</span> (<u
                  style="font-weight: bold;">C</u>omponent-Focused <u style="font-weight: bold;">A</u>daptive
                <u style="font-weight: bold;">L</u>earning for Multi-<u style="font-weight: bold;">I</u>mage <u
                  style="font-weight: bold;">C</u>o-Localization of <u style="font-weight: bold;">O</u>bjects),
                an LVLM designed for part-focused semantic co-segmentation. <span class="model-name">Calico</span>
                incorporates a novel correspondence
                extraction module to learn cross-image semantic correspondences and an
                adaptation module to enable localized co-segmentation across multiple images in a parameter-efficient
                manner.
              </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>New Dataset</b>. We
                introduce the <span class="model-name">MixedParts</span> dataset for
                part-focused
                semantic co-segmentation, compiled from diverse part
                segmentation datasets and featuring images of logically
                comparable objects and parts.</li>
              </ol>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name">Calico</span> Architecture</h2>
          <div class="content has-text-justified">
            <img src="./static/images/calico_arch.svg" class="interpolation-image" alt="CALICO Model Architecture." />
            <div class="horizontal-container">
              <p><span class="model-name">Calico</span> uses a Q-Former cross-attention module to query efficient image
                embeddings from a pretrained image encoder, which are passed
                into a Vicuna-based LLM
                as image features. We extract <code>[SEG]</code> tokens from the output text, which are used to prompt a
                SAM decoder to output corresponding
                segmentation masks. <br> <br> We propose two modules, the Correspondence Extraction Module (CEM) and the
                Correspondence Adaptation
                Module (CAM), to enable the learning of semantic-rich features for multi-image correspondence. In
                <span class="model-name">Calico</span>, k CAMs are strategically placed every N/k layers within the
                N-layered LLM. CEM focuses on
                extracting fine-grained semantic information at the part level, capturing correspondences across similar
                yet distinct object categories by leveraging self-supervised DINO features. CAMs then reintegrate this
                part-level correspondence information back into the next layer of the model.
              </p>
              <img src="./static/images/cemcam.svg" class="interpolation-image"
                alt="CALICO novel modules CEM and CAM." />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name">MixedParts</span> Dataset</h2>
          <div class="content has-text-justified">
            <p>
              Although multi-image datasets of various scales are available, they exhibit combinations of limitations,
              making them
              unsuitable for the part-focused semantic co-segmentation
              task. Limitations include the absence of fine-grained
              masks for segmentation, datasets being too small or domain-specific to facilitate generalizable LVLM
              training despite containing localized labels, or the lack of part-level information altogether.</p>

            <p>To address these
              challenges and enable effective training and evaluation of our part-focused semantic
              co-segmentation model, we introduce a novel dataset named
              <span class="model-name">MixedParts</span>, curated from publicly available part segmentation datasets:
              <a href="https://github.com/OpenRobotLab/OV_PARTS">ADE20K-Part234</a>, <a
                href="https://github.com/facebookresearch/paco">PACO</a>, and <a
                href="https://github.com/TACJu/PartImageNet">PartImageNet</a>.
            </p>
            <img src="./static/images/mixed_parts_example.png" class="interpolation-image"
              alt="MixedParts dataset examples." />

            <p>Example image pairs in <span class="model-name">MixedParts</span> with objects, common parts, and unique
              parts segmented and labeled. Each
              column represents a different image pair, derived from a set of diverse datasets with various levels of
              detail, PACO, PartImageNet, and
              ADE20K-Part-234, covering both rigid and non-rigid objects and parts. Each image pair is displayed across
              3 rows to illustrate (i) the
              (possibly common) object, (ii) the common object parts, and (iii) the unique object parts in each pair.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Quantitative Results</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; padding: 0 0 20px 0;">
              <img src="./static/images/results.png" class="interpolation-image" alt="Calico results." width="85%" />
              <p class="has-text-justified">
                Experimental Results on <span class="model-name">MixedParts</span>. The first three
                metrics are segmentation-based, while the last two are text-based.
                <span class="model-name">Calico</span> outperforms baselines across all metrics.
              </p>

            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Qualitative Results</h2>
          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_object_object_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_object_object_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The common object is <span
                class="highlight one">the&nbsp;person</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_object_object_2.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_object_object_3.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images include
              <span class="highlight two">a&nbsp;car</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container-container">
              <div class="image-container">
                <img src="static/images/qualitatives/preds_common_part_object_0.png" alt="Image 1" class="image">
                <img src="static/images/qualitatives/preds_common_part_object_1.png" alt="Image 2" class="image">
              </div>
              <div class="image-container">
                <img src="static/images/qualitatives/preds_common_part_parts_0.png" alt="Image 1" class="image">
                <img src="static/images/qualitatives/preds_common_part_parts_1.png" alt="Image 2" class="image">
              </div>
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images show <span
                class="highlight three">a&nbsp;snake</span> and <span class="highlight four">a&nbsp;dog</span>.<br>The
              detected common parts are <span class="highlight five">a&nbsp;body</span> and <span
                class="highlight six">a&nbsp;head</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_unique_part_object_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_unique_part_object_1.png" alt="Image 2" class="image">
            </div>
            <div class="image-container">
              <img src="static/images/qualitatives/preds_unique_part_parts_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_unique_part_parts_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images show <span
                class="highlight seven">a&nbsp;bed</span> and <span
                class="highlight eight">a&nbsp;pool&nbsp;table</span>.<br>The
              unique parts present are <span class="highlight nine">a&nbsp;headboard</span>,
              <span class="highlight ten">a&nbsp;bed</span>, and <span class="highlight eleven">a&nbsp;pocket</span>.
            </p>
          </div>

          <p class="has-text-justified" style="padding-bottom:40px;">
            <span class="model-name">Calico</span> demonstrates pixel-grounded understanding of various non-rigid (e.g.,
            humans, animals)
            and rigid objects (e.g., car, bed), including less common objects and their parts (e.g., the bed and pocket
            of a pool table).
          </p>

          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/in_context_0_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/in_context_0_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images show <span
                class="highlight twelve">a&nbsp;dog</span>.
            </p>
            <hr style="background-color:LightGray;margin:50px;">
            <div class="image-container">
              <img src="static/images/qualitatives/in_context_1_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/in_context_1_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images include <span
                class="highlight thirteen">a&nbsp;car</span>.
            </p>
          </div>
          <p class="has-text-justified">
            <span class="model-name">Calico</span> outputs are highly context-driven when distinguishing objects
            across
            images, despite variations in angle, size, saliency, etc. Different image pairings prompt the
            model to segment different objects accordingly, rather than
            defaulting to the most salient object in each image.
          </p>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{nguyen2025calico,
  title={CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models},
  author={Nguyen, Kiet A. and Juvekar, Adheesh and Yu, Tianjiao and Wahed, Muntasir and Lourentzou, Ismini},
  journal={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2412.19331">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/PLAN-Lab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This site is built upon the work of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              made available under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>. We gratefully acknowledge <a
                href="https://arxiv.org/pdf/2304.08485">LLaVA</a>,
              <a href="https://arxiv.org/pdf/2311.03356">GLaMM</a>,
              <a href="https://arxiv.org/pdf/2304.07193">DINOv2</a>, <a
                href="https://arxiv.org/pdf/2301.12597">Q-Former</a>, and <a
                href="https://arxiv.org/pdf/2308.04152">Cheetah</a> for open-sourcing their models
              and code.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>