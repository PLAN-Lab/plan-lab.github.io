<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models">
  <meta name="keywords" content="part segmentation, part co-segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>[CVPR 2025] CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/calico.svg" alt="CALICO cat"
                width="80px"><span class="model-name">Calico</span>: Part-Focused Semantic Co-Segmentation with Large
              Vision-Language Models</h1>
            <h2 class="title is-1 publication-title illini-orange">Accepted to CVPR 2025</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kanguyen-vn/" class="illini-blue">Kiet A. Nguyen</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/juvekaradheesh/" class="illini-blue">Adheesh Juvekar</a>,</span>
              <span class="author-block">
                <a href="https://tianjiao-yu.github.io/" class="illini-blue">Tianjiao Yu</a>,
              </span>
              <span class="author-block">
                <a href="https://mwahed.com/" class="illini-blue">Muntasir Wahed</a>,
              </span>
              <span class="author-block">
                <a href="https://isminoula.github.io/" class="illini-blue">Ismini Lourentzou</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">PLAN Lab</span>,
              <span class="author-block illini-orange">University of Illinois Urbana-Champaign</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.19331" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.19331" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data (coming soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="." class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-vr-cardboard"></i>
                    </span>
                    <span>Demo (coming soon)</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="content has-text-justified">
          <span style="font-weight: bold;">TL;DR:</span> We present <span class="model-name">Calico</span>, the first
          LVLM
          designed for <span style="font-style: italic;">part-focused semantic co-segmentation</span>, a new task that
          identifies and
          segments common and unique object parts across multiple images. Trained on <span
            class="model-name">MixedParts</span>, our new dataset with ~2.4M samples across ~44K images,
          <span class="model-name">Calico</span> achieves strong
          performance in this domain with just 0.3% of its architecture finetuned.
        </div>
        <img src="./static/images/calico_teaser.svg" class="interpolation-image"
          alt="Interpolate start reference image." />
        <div class="content has-text-justified">
          Our proposed <span style="font-style: italic;">part-focused semantic
            co-segmentation</span> task, where the goal is to identify, segment, and label
          common objects, as well as common and unique object parts across multiple images.
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advances in Large Vision-Language Models (LVLMs) have sparked significant progress in
              general-purpose vision tasks through visual instruction tuning. While some works have demonstrated the
              capability of LVLMs to generate segmentation masks that align phrases with natural language descriptions
              in a single image, they struggle with segmentation-grounded comparisons across multiple images,
              particularly at finer granularities such as object parts. In this paper, we introduce the new task of
              <span style="font-style: italic; font-weight: bold;">part-focused semantic co-segmentation</span>, which
              seeks to identify
              and segment common and unique
              objects and parts across multiple images.
            </p>

            <p>To address this task, we present <span class="model-name">Calico</span>, the first LVLM
              that can segment and reason over multiple masks across images, enabling object comparison based on their
              constituent parts. <span class="model-name">Calico</span> features two proposed components, a novel
              Correspondence Extraction
              Module, which captures semantic-rich information to identify part-level correspondences between objects,
              and a Correspondence Adaptation Module, which embeds this information into the LLM and facilitates
              multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate
              <span class="model-name">MixedParts</span>, a comprehensive multi-image segmentation dataset containing
              ~2.4M samples across
              ~44K images with diverse object and part categories. Experimental results show <span
                class="model-name">Calico</span>,
              finetuned on only 0.3% of its architecture, achieves robust performance in part-focused semantic
              co-segmentation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" style="background-color:#dadada81">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">âœ… Contributions</h2>
          <div class="content has-text-justified">
            <ul class="fa-ul">
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>Novel Task</b>. We introduce
                the novel task of <span style="font-style: italic;">part-focused
                  semantic co-segmentation</span>, which aims to co-segment and label common and unique parts between
                objects across images for granular object comparison. To the best of our
                knowledge, this is the first work to formalize this multi-image object/part co-segmentation task. </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>New Multi-Image
                  Pixel-Grounded LVLM</b>. We propose <span class="model-name">Calico</span> (<u
                  style="font-weight: bold;">C</u>omponent-Focused <u style="font-weight: bold;">A</u>daptive
                <u style="font-weight: bold;">L</u>earning for Multi-<u style="font-weight: bold;">I</u>mage <u
                  style="font-weight: bold;">C</u>o-Localization of <u style="font-weight: bold;">O</u>bjects),
                an LVLM designed for part-focused semantic co-segmentation. CALICO incorporates a novel correspondence
                extraction module to learn cross-image semantic correspondences and an
                adaptation module to enable localized co-segmentation across multiple images in a parameter-efficient
                manner.
              </li>
              <br>
              <li><span class="fa-li"><i class="fa fa-paw"></i></span><b>New Dataset</b>. We
                introduce the <span class="model-name">MixedParts</span> dataset for
                part-focused
                semantic co-segmentation, compiled from diverse part
                segmentation datasets and featuring images of logically
                comparable objects and parts.</li>
              </ol>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name">Calico</span> Architecture</h2>
          <div class="content has-text-justified">
            <img src="./static/images/calico_arch.svg" class="interpolation-image" alt="CALICO Model Architecture." />
            <div class="horizontal-container">
              <p><span class="model-name">Calico</span> uses a Q-Former cross-attention module to query efficient image
                embeddings from a pretrained image encoder, which are passed
                into a Vicuna-based LLM
                as image features. We extract <code>[SEG]</code> tokens from the output text, which are used to prompt a
                SAM decoder to output corresponding
                segmentation masks. <br> <br> We propose two modules, the Correspondence Extraction Module (CEM) and the
                Correspondence Adaptation
                Module (CAM), to enable the learning of semantic-rich features for multi-image correspondence. In
                <span class="model-name">Calico</span>, k CAMs are strategically placed every N/k layers within the
                N-layered LLM. CEM focuses on
                extracting fine-grained semantic information at the part level, capturing correspondences across similar
                yet distinct object categories by leveraging self-supervised DINO features. CAMs then reintegrate this
                part-level correspondence information back into the next layer of the model.
              </p>
              <img src="./static/images/cemcam.svg" class="interpolation-image"
                alt="CALICO novel modules CEM and CAM." />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3"><span class="model-name">MixedParts</span> Dataset</h2>
          <div class="content has-text-justified">
            <p>
              Although multi-image datasets of various scales are available, they exhibit combinations of limitations,
              making them
              unsuitable for the part-focused semantic co-segmentation
              task. Limitations include the absence of fine-grained
              masks for segmentation, datasets being too small or domain-specific to facilitate generalizable LVLM
              training despite containing localized labels, or the lack of part-level information altogether.</p>

            <p>To address these
              challenges and enable effective training and evaluation of our part-focused semantic
              co-segmentation model, we introduce a novel dataset named
              <span class="model-name">MixedParts</span>, curated from publicly available part segmentation datasets:
              <a href="https://github.com/OpenRobotLab/OV_PARTS">ADE20K-Part234</a>, <a
                href="https://github.com/facebookresearch/paco">PACO</a>, and <a
                href="https://github.com/TACJu/PartImageNet">PartImageNet</a>.
            </p>
            <img src="./static/images/mixed_parts_example.png" class="interpolation-image"
              alt="MixedParts dataset examples." />

            <p>Example image pairs in <span class="model-name">MixedParts</span> with objects, common parts, and unique
              parts segmented and labeled. Each
              column represents a different image pair, derived from a set of diverse datasets with various levels of
              detail, PACO, PartImageNet, and
              ADE20K-Part-234, covering both rigid and non-rigid objects and parts. Each image pair is displayed across
              3 rows to illustrate (i) the
              (possibly common) object, (ii) the common object parts, and (iii) the unique object parts in each pair.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Quantitative Results</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; padding: 0 0 20px 0;">
              <img src="./static/images/results.png" class="interpolation-image" alt="Calico results." width="85%" />
              <p class="has-text-justified">
                Experimental Results on <span class="model-name">MixedParts</span>. The first three
                metrics are segmentation-based, while the last two are text-based.
                <span class="model-name">Calico</span> outperforms baselines across all metrics.
              </p>

            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Qualitative Results</h2>
          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_object_object_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_object_object_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The common object is <span
                class="highlight one">the&nbsp;person</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_object_object_2.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_object_object_3.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The object present in the images is
              <span class="highlight two">the&nbsp;car</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container-container">
              <div class="image-container">
                <img src="static/images/qualitatives/preds_common_part_object_0.png" alt="Image 1" class="image">
                <img src="static/images/qualitatives/preds_common_part_object_1.png" alt="Image 2" class="image">
              </div>
              <div class="image-container">
                <img src="static/images/qualitatives/preds_common_part_parts_0.png" alt="Image 1" class="image">
                <img src="static/images/qualitatives/preds_common_part_parts_1.png" alt="Image 2" class="image">
              </div>
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images show <span
                class="highlight three">a&nbsp;snake</span> and <span class="highlight four">a&nbsp;dog</span>.<br>The
              common parts in both objects are <span class="highlight five">the&nbsp;body</span> and <span
                class="highlight six">the&nbsp;head</span>.
            </p>
          </div>

          <div class="qual-example">
            <div class="image-container">
              <img src="static/images/qualitatives/preds_unique_part_object_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_unique_part_object_1.png" alt="Image 2" class="image">
            </div>
            <div class="image-container">
              <img src="static/images/qualitatives/preds_unique_part_parts_0.png" alt="Image 1" class="image">
              <img src="static/images/qualitatives/preds_unique_part_parts_1.png" alt="Image 2" class="image">
            </div>
            <p class="caption has-text-centered">
              <img src="static/images/calico.svg" alt="CALICO cat" width="40px">: The images contain <span
                class="highlight seven">a&nbsp;bed</span> and <span class="highlight eight">a&nbsp;table</span>.<br>The
              unique parts between the objects are <span class="highlight nine">a&nbsp;headboard</span>,
              <span class="highlight ten">a&nbsp;top</span>, and <span class="highlight eleven">a&nbsp;leg</span>.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{nguyen2025calico,
  title={CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models},
  author={Nguyen, Kiet A. and Juvekar, Adheesh and Yu, Tianjiao and Wahed, Muntasir and Lourentzou, Ismini},
  journal={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2412.19331">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/PLAN-Lab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This site is built upon the work of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              made available under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>. We gratefully acknowledge <a
                href="https://arxiv.org/pdf/2304.08485">LLaVA</a>,
              <a href="https://arxiv.org/pdf/2311.03356">GLaMM</a>,
              <a href="https://arxiv.org/pdf/2304.07193">DINOv2</a>, <a
                href="https://arxiv.org/pdf/2301.12597">Q-Former</a>, and <a
                href="https://arxiv.org/pdf/2308.04152">Cheetah</a> for open-sourcing their models
              and code.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>